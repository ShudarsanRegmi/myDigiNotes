## âœ¦ Goal:

> Convert a Unicode code point (like `U+1F604` for ğŸ˜„) into its corresponding **UTF-8 byte sequence**.

---

## ğŸ§  Step 1: Understand UTF-8 Byte Patterns

UTF-8 uses **variable-length encoding**, and each code point is packed into 1 to 4 bytes depending on its value.

### Byte structure (based on range of code point):

| Code point (hex)       | Byte Count | UTF-8 Format                          |
| ---------------------- | ---------- | ------------------------------------- |
| `U+0000` â€“ `U+007F`    | 1 byte     | `0xxxxxxx`                            |
| `U+0080` â€“ `U+07FF`    | 2 bytes    | `110xxxxx 10xxxxxx`                   |
| `U+0800` â€“ `U+FFFF`    | 3 bytes    | `1110xxxx 10xxxxxx 10xxxxxx`          |
| `U+10000` â€“ `U+10FFFF` | 4 bytes    | `11110xxx 10xxxxxx 10xxxxxx 10xxxxxx` |

Each `x` is a bit from the original Unicode code point.

---

## âœ¦ Step 2: General Encoding Procedure

Letâ€™s define the process formally:

1. Get the **binary value** of the Unicode code point.
2. Figure out **how many bytes** it needs based on its range.
3. Break the binary into **chunks** and insert into the correct template above.
4. Convert final bit patterns into **hex bytes**.

---

## âœ¦ Step 3: Example 1 â€” Convert `U+0041` (letter `A`)

1. Binary of `U+0041`: `00000000 01000001` â†’ just `01000001`
2. It fits in the 1-byte range `U+0000` to `U+007F`
3. UTF-8 format: `0xxxxxxx`
4. So:

   * `A` â†’ `01000001` â†’ `0x41` â†’ UTF-8 = `41`

âœ… Done.

---

## âœ¦ Step 4: Example 2 â€” Convert `U+00E9` (Ã©)

1. Binary: `11101001` â†’ fits in `U+0080` to `U+07FF` â†’ 2 bytes

2. UTF-8 template: `110xxxxx 10xxxxxx`

3. 11 bits of data: `00011101001`
   (pad left if needed: total 11 bits) (total 11 bits is required as there are 11 xes. Start filling from first(left byte))

4. Split into:

   * First 5 bits: `00011`
   * Last 6 bits: `101001`

5. Fill the template:

   * Byte 1: `11000011` â†’ `0xC3`
   * Byte 2: `10101001` â†’ `0xA9`

âœ… UTF-8 bytes = `C3 A9`

---

## âœ¦ Step 5: Example 3 â€” Convert `U+1F604` (ğŸ˜„)

1. Hex: `0x1F604`
   Binary (21 bits): `0001 1111 0110 0000 0100`

2. Needs 4 bytes: `11110xxx 10xxxxxx 10xxxxxx 10xxxxxx`

Split the 21 bits: (21 bits because we need to fill in place of 21xes. Start filling from first (left byte))

* Chunk 1 (first 3 bits): `00001`
* Chunk 2 (next 6 bits): `111101`
* Chunk 3: `100000`
* Chunk 4: `0100`

Rewriting cleanly:

```
Full 21-bit: 00011111011000000100  
Split into:
  xxxxx  = 00001  â†’ goes into 1st byte (last 3 bits)
  xxxxxx = 111101 â†’ 2nd byte
  xxxxxx = 100000 â†’ 3rd byte
  xxxxxx = 000100 â†’ 4th byte
```

Final filled template:

* `11110000` (first byte, `11110xxx` with `00001`)
* `10011111`
* `10011000`
* `10000100`

âœ… UTF-8 bytes = `F0 9F 98 84`

---

## âœ¦ Bonus: Python Code to See It

You can verify any character's UTF-8 byte sequence using Python:

```python
char = 'ğŸ˜„'
utf8_bytes = char.encode('utf-8')
print(list(utf8_bytes))  # Output: [240, 159, 152, 132]
```

Or for a codepoint:

```python
codepoint = 0x1F604
print(chr(codepoint).encode('utf-8'))  # b'\xf0\x9f\x98\x84'
```

---

## ğŸ” Quick Summary:

| Code Point Range   | UTF-8 Bytes | Example        | Output        |
| ------------------ | ----------- | -------------- | ------------- |
| U+0000 â€“ U+007F    | 1           | `U+0041` (A)   | `41`          |
| U+0080 â€“ U+07FF    | 2           | `U+00E9` (Ã©)   | `C3 A9`       |
| U+0800 â€“ U+FFFF    | 3           | `U+20AC` (â‚¬)   | `E2 82 AC`    |
| U+10000 â€“ U+10FFFF | 4           | `U+1F604` (ğŸ˜„) | `F0 9F 98 84` |

---


Letâ€™s gooo â€” now you're asking for the reverse process:

---

### ğŸ’¬ Rephrased:

> How do I decode a UTF-8 byte sequence back into its original Unicode code point?

This is the other half of the story â€” just as important as encoding. Letâ€™s build a clear method you can follow, then apply it with some hands-on examples.

---

## ğŸ§  Step-by-Step: Decoding UTF-8 Bytes â†’ Unicode Code Point

### âœ¦ Step 1: Identify the number of bytes

Look at the **first byte** in binary:

| Leading Bits | Byte Count | Meaning                 |
| ------------ | ---------- | ----------------------- |
| `0xxxxxxx`   | 1 byte     | ASCII (U+0000 â€“ U+007F) |
| `110xxxxx`   | 2 bytes    | U+0080 â€“ U+07FF         |
| `1110xxxx`   | 3 bytes    | U+0800 â€“ U+FFFF         |
| `11110xxx`   | 4 bytes    | U+10000 â€“ U+10FFFF      |

All continuation bytes (after the first one) **start with `10xxxxxx`**.

---

### âœ¦ Step 2: Remove prefix bits to get raw payload

You **remove the prefix bits** from each byte and then **concatenate all remaining bits** to reconstruct the Unicode code point.

---

### âœ¦ Step 3: Convert final binary into hex â†’ that's your Unicode code point.

---

## âœ… Example 1: Decode `C3 A9`

This is the UTF-8 encoding for `Ã©`

### Step 1: Convert to binary

* `C3` = `11000011`
* `A9` = `10101001`

### Step 2: Remove prefixes

* From `11000011`: remove `110` â†’ `00011`
* From `10101001`: remove `10` â†’ `101001`

Now concatenate:

```
00011 101001 â†’ 00011101001
```

This is **11 bits**: binary `00011101001` â†’ decimal `233` â†’ hex `0x00E9`

âœ… So: `C3 A9` = `U+00E9` = `Ã©`

---

## âœ… Example 2: Decode `F0 9F 98 84`

This is the UTF-8 encoding for `ğŸ˜„`

### Step 1: Convert to binary

| Byte | Binary     |
| ---- | ---------- |
| F0   | `11110000` |
| 9F   | `10011111` |
| 98   | `10011000` |
| 84   | `10000100` |

### Step 2: Remove prefixes

* `F0`: remove `11110` â†’ `000`
* `9F`: remove `10` â†’ `011111`
* `98`: remove `10` â†’ `011000`
* `84`: remove `10` â†’ `000100`

Now concatenate:

```
000 011111 011000 000100 â†’ 00011111011000000100
```

This is a **21-bit binary number** â†’ `0001 1111 0110 0000 0100`

Convert to hex:

* Binary: `00011111011000000100`
* Decimal: 128516
* Hex: `0x1F604`

âœ… So: `F0 9F 98 84` = `U+1F604` = ğŸ˜„

---

## ğŸ§  Common Mistakes to Avoid:

* Donâ€™t count total byte length as bit length â€” only use the payload bits.
* Always strip off prefix bits: `110`, `1110`, `11110`, and `10`.
* Donâ€™t convert each byte to hex and concatenate â€” that **wonâ€™t work**.

---

## ğŸ” Summary: Reverse UTF-8 Decoding Steps

1. Read bytes â†’ Convert to binary
2. Identify byte count from first byte
3. Remove UTF-8 prefix bits (`110`, `10`, etc.)
4. Concatenate payload bits
5. Convert full binary to hex â†’ that's your code point

---
